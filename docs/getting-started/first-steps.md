# Your First Evaluation

This guide walks you through your first evaluation using the LLM-as-a-Judge system.

## ğŸ¯ What You'll Learn

- How to evaluate a single response
- How to compare two responses
- Understanding the output format
- Basic configuration options

## Step 1: Single Response Evaluation

### Basic Evaluation

```bash
# Evaluate a single response
python -m src.llm_judge evaluate "What is AI?" "AI is artificial intelligence"
```

**Expected Output:**

```
ğŸ¯ Multi-Criteria LLM Evaluation Results
================================================================================
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Overall Score â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ 3.8/5.0                                             â”‚
â”‚ Confidence: 88.5%                                   â”‚
â”‚ Based on 7 criteria                                 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

ğŸ“Š Detailed Criterion Scores
â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”³â”â”â”â”â”â”³â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ Criterion  â”ƒ Scâ€¦â”ƒ Weâ€¦ â”ƒ Conâ€¦ â”ƒ Reasoning          â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â•‡â”â”â”â”â”â•‡â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚ accuracy   â”‚ 4â€¦â”‚ 20â€¦ â”‚ 90.â€¦ â”‚ Factually correct  â”‚
â”‚ clarity    â”‚ 4â€¦â”‚ 15â€¦ â”‚ 85.â€¦ â”‚ Well articulated   â”‚
â”‚ relevance  â”‚ 5â€¦â”‚ 15â€¦ â”‚ 95.â€¦ â”‚ Directly addresses â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

âœ… Strengths: Clear, accurate, relevant
âš ï¸  Areas for Improvement: Lacks depth, needs examples
ğŸ’¡ Suggestions: Add more detail, provide context
```

### Understanding the Output

- **Overall Score**: Weighted average across all criteria (1-5 scale)
- **Confidence**: Judge's confidence in the assessment (0-100%)
- **Criterion Scores**: Individual scores for each evaluation dimension
- **Strengths/Weaknesses**: Identified areas of excellence and improvement
- **Suggestions**: Actionable recommendations for enhancement

## Step 2: Response Comparison

### Compare Two Responses

```bash
# Compare two responses
python -m src.llm_judge compare "Explain machine learning" \
  "ML is a subset of AI" \
  "Machine learning is a subset of artificial intelligence that enables computers to learn from data without being explicitly programmed"
```

**Expected Output:**

```
ğŸ† Response Comparison Results
================================================================================
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Winner â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Response B                                       â”‚
â”‚ Confidence: 92.3%                               â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

ğŸ“Š Comparison Analysis
Response B demonstrates superior:
â€¢ Completeness: Provides comprehensive definition
â€¢ Accuracy: Technically precise terminology
â€¢ Clarity: Well-structured explanation
â€¢ Helpfulness: More actionable information

Response A shows:
â€¢ Brevity: Concise but lacks detail
â€¢ Accuracy: Correct but incomplete
```

## Step 3: Custom Configuration

### Using Different Models

```bash
# Use specific judge model
python -m src.llm_judge --judge-model claude-3 evaluate "Question" "Answer"

# Use different provider
python -m src.llm_judge --provider openai evaluate "Question" "Answer"
```

### Output Formats

```bash
# JSON output for programmatic use
python -m src.llm_judge --output json evaluate "Question" "Answer"

# Verbose output with detailed information
python -m src.llm_judge --verbose evaluate "Question" "Answer"
```

## Step 4: Batch Processing

### Create Sample Batch

```bash
# Create a sample batch file
python -m src.llm_judge create-sample-batch sample.jsonl
```

### Process Batch

```bash
# Process the batch file
python -m src.llm_judge batch sample.jsonl --output results.json
```

## ğŸ‰ Congratulations!

You've successfully completed your first evaluations! Here's what you've learned:

- âœ… How to evaluate single responses
- âœ… How to compare multiple responses
- âœ… Understanding the output format
- âœ… Basic configuration options
- âœ… Batch processing basics

## Next Steps

- **[Configuration Guide](../configuration/README.md)** - Advanced configuration options
- **[API Reference](../api/README.md)** - Programmatic usage
- **[Examples](../examples/README.md)** - More complex examples
- **[Architecture Overview](../architecture/README.md)** - Understanding the system

## ğŸ†˜ Need Help?

- Check the [FAQ](../overview/README.md#faq)
- Review [Common Issues](../overview/README.md#common-issues)
- [Open an issue](https://github.com/superluminal-jp/llm-as-a-judge/issues)
