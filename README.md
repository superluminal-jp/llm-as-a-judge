# LLM-as-a-Judge System

[![Tests](https://img.shields.io/badge/tests-359%20passing-brightgreen)](https://github.com/superluminal-jp/llm-as-a-judge)
[![Python](https://img.shields.io/badge/python-3.9%2B-blue)](https://python.org)
[![License](https://img.shields.io/badge/license-MIT-green)](LICENSE)
[![Architecture](https://img.shields.io/badge/architecture-DDD%20Clean%20Architecture-purple)](docs/architecture/README.md)

A comprehensive LLM-as-a-Judge system for evaluating language model outputs with **multi-criteria evaluation by default**. Features comprehensive scoring across 7 evaluation dimensions with rich CLI interface, robust batch processing capabilities, **structured output support** across all providers, and **specialized domain-specific evaluation criteria** including administrative information disclosure assessment.

## Quick Start

```bash
# Install dependencies
pip install -r requirements.txt

# Set up environment (optional - for real LLM integration)
cp .env.example .env
# Edit .env with your API keys

# CLI Usage - Multi-criteria evaluation (default)
python -m src.llm_judge evaluate "What is AI?" "AI is artificial intelligence"

# CLI Usage - Administrative Information Disclosure Evaluation (Featured)
python -m src.llm_judge evaluate "è¡Œæ”¿æ–‡æ›¸ã®é–‹ç¤ºå¯å¦ã«ã¤ã„ã¦" "ã“ã®æ–‡æ›¸ã«ã¯å€‹äººæƒ…å ±ãŒå«ã¾ã‚Œã¦ã„ã‚‹ãŸã‚..." --criteria-file criteria/administrative_information_non_disclosure.json

# CLI Usage - Custom criteria weights
python -m src.llm_judge evaluate "What is AI?" "AI is artificial intelligence" --criteria-weights "accuracy:0.4,clarity:0.3,helpfulness:0.3"

# CLI Usage - Equal weights for all criteria (now the default)
python -m src.llm_judge evaluate "What is AI?" "AI is artificial intelligence" --equal-weights

# CLI Usage - Custom criteria definition
python -m src.llm_judge evaluate "What is AI?" "AI is artificial intelligence" --custom-criteria "accuracy:Factual correctness:factual:0.4,clarity:How clear the response is:linguistic:0.3,helpfulness:How useful the response is:qualitative:0.3"

# CLI Usage - Custom criteria from file
python -m src.llm_judge evaluate "What is AI?" "AI is artificial intelligence" --criteria-file criteria/default.json

# CLI Usage - List available criteria types
python -m src.llm_judge evaluate --list-criteria-types

# CLI Usage - Create criteria template
python -m src.llm_judge create-criteria-template my-criteria.json --name "Academic Evaluation" --description "Criteria for academic content"

# CLI Usage - Data management
python -m src.llm_judge data list --limit 10 --format table
python -m src.llm_judge data export results.json --limit 100
python -m src.llm_judge data clean-cache --force

# CLI Usage - Single-criterion evaluation (legacy)
python -m src.llm_judge evaluate "What is AI?" "AI is artificial intelligence" --single-criterion --criteria "accuracy"

# CLI Usage - Compare two responses
python -m src.llm_judge compare "Explain ML" "Basic explanation" "Detailed explanation" --model-a gpt-4 --model-b claude-3

# CLI Usage - Batch processing from file
python -m src.llm_judge create-sample-batch sample.jsonl  # Create sample file
python -m src.llm_judge batch sample.jsonl --output results.json --max-concurrent 5

# CLI Usage - Batch processing with administrative criteria
python -m src.llm_judge batch administrative_batch.jsonl --output admin_results.json --criteria-file criteria/administrative_information_non_disclosure.json

# Run tests
pytest

# Run specific test suites
pytest tests/unit/                    # Unit tests only
pytest tests/integration/             # Integration tests only
pytest tests/unit/infrastructure/     # Infrastructure layer tests
```

## ğŸ“– Documentation

### ğŸ¯ For Different Audiences

| **I want to...**          | **Start here**                                      | **Then explore**                              |
| ------------------------- | --------------------------------------------------- | --------------------------------------------- |
| **Get started quickly**   | [Quick Start Guide](docs/getting-started/README.md) | [Configuration](docs/configuration/README.md) |
| **Understand the system** | [System Overview](docs/overview/README.md)          | [Architecture](docs/architecture/README.md)   |
| **Use the API**           | [API Reference](docs/api/README.md)                 | [Examples](docs/examples/README.md)           |
| **Develop features**      | [Development Guide](docs/development/README.md)     | [Testing](docs/testing/README.md)             |
| **Deploy/Operate**        | [Deployment Guide](docs/deployment/README.md)       | [Configuration](docs/configuration/README.md) |

### ğŸ“š Complete Documentation Structure

```
docs/
â”œâ”€â”€ getting-started/          # ğŸš€ Quick start and basic usage
â”‚   â”œâ”€â”€ README.md            # Quick start guide
â”‚   â”œâ”€â”€ installation.md      # Installation instructions
â”‚   â””â”€â”€ first-steps.md       # Your first evaluation
â”œâ”€â”€ overview/                # ğŸ¯ System overview and concepts
â”‚   â”œâ”€â”€ README.md            # System overview
â”‚   â”œâ”€â”€ features.md          # Key features and capabilities
â”‚   â””â”€â”€ concepts.md          # Core concepts and terminology
â”œâ”€â”€ api/                     # ğŸ“– API documentation
â”‚   â”œâ”€â”€ README.md            # API overview
â”‚   â”œâ”€â”€ reference.md         # Complete API reference
â”‚   â””â”€â”€ examples.md          # Code examples and patterns
â”œâ”€â”€ configuration/           # âš™ï¸ Configuration and setup
â”‚   â”œâ”€â”€ README.md            # Configuration overview
â”‚   â”œâ”€â”€ environment.md       # Environment setup
â”‚   â””â”€â”€ advanced.md          # Advanced configuration
â”œâ”€â”€ architecture/            # ğŸ—ï¸ System architecture
â”‚   â”œâ”€â”€ README.md            # Architecture overview
â”‚   â”œâ”€â”€ design.md            # Design principles and patterns
â”‚   â””â”€â”€ components.md        # Component details
â”œâ”€â”€ development/             # ğŸ‘¨â€ğŸ’» Development guides
â”‚   â”œâ”€â”€ README.md            # Development overview
â”‚   â”œâ”€â”€ setup.md             # Development environment
â”‚   â”œâ”€â”€ contributing.md      # Contributing guidelines
â”‚   â””â”€â”€ code-standards.md    # Coding standards
â”œâ”€â”€ testing/                 # ğŸ§ª Testing documentation
â”‚   â”œâ”€â”€ README.md            # Testing overview
â”‚   â”œâ”€â”€ running-tests.md     # How to run tests
â”‚   â””â”€â”€ writing-tests.md     # How to write tests
â”œâ”€â”€ deployment/              # ğŸš€ Deployment and operations
â”‚   â”œâ”€â”€ README.md            # Deployment overview
â”‚   â”œâ”€â”€ production.md        # Production deployment
â”‚   â””â”€â”€ monitoring.md        # Monitoring and observability
â””â”€â”€ examples/                # ğŸ’¡ Examples and tutorials
    â”œâ”€â”€ README.md            # Examples overview
    â”œâ”€â”€ basic-usage.md       # Basic usage examples
    â”œâ”€â”€ advanced-usage.md    # Advanced usage examples
    â””â”€â”€ integration.md       # Integration examples
```

## ğŸ—ï¸ Project Structure

The project follows Domain-Driven Design (DDD) principles with clean architecture:

```
llm-as-a-judge/
â”œâ”€â”€ src/llm_judge/                    # Main application package
â”‚   â”œâ”€â”€ __init__.py                  # Package exports (LLMJudge, CandidateResponse, etc.)
â”‚   â”œâ”€â”€ __main__.py                  # CLI entry point
â”‚   â”œâ”€â”€ domain/                      # Business logic and domain models
â”‚   â”‚   â”œâ”€â”€ evaluation/              # Multi-criteria evaluation domain logic
â”‚   â”‚   â”œâ”€â”€ batch/                   # Batch processing domain logic
â”‚   â”‚   â””â”€â”€ models/                  # Domain models and value objects
â”‚   â”œâ”€â”€ application/                 # Use cases and application services
â”‚   â”‚   â”œâ”€â”€ services/                # Application services
â”‚   â”‚   â””â”€â”€ use_cases/               # Specific use case implementations
â”‚   â”œâ”€â”€ infrastructure/              # External concerns and technical details
â”‚   â”‚   â”œâ”€â”€ clients/                 # LLM provider API clients
â”‚   â”‚   â”œâ”€â”€ config/                  # Configuration management
â”‚   â”‚   â”œâ”€â”€ resilience/              # Reliability and resilience patterns
â”‚   â”‚   â””â”€â”€ logging/                 # Logging infrastructure
â”‚   â””â”€â”€ presentation/                # User interfaces
â”‚       â””â”€â”€ cli/                     # Command-line interface
â”œâ”€â”€ tests/                           # Test suite organized by layer
â”‚   â”œâ”€â”€ unit/                        # Unit tests (isolated, fast)
â”‚   â”œâ”€â”€ integration/                 # Integration tests (cross-layer)
â”‚   â””â”€â”€ fixtures/                    # Test fixtures and sample data
â”œâ”€â”€ docs/                            # Comprehensive documentation
â”œâ”€â”€ criteria/                        # Evaluation criteria configuration files
â”‚   â””â”€â”€ administrative_information_non_disclosure.json  # Featured: Legal compliance evaluation
â”œâ”€â”€ config.json                      # Main configuration file
â””â”€â”€ README.md                        # This file - project overview
```

## ğŸ¯ Key Features

### âœ… Multi-Criteria Evaluation (Default)

- **7 evaluation criteria**: accuracy, completeness, clarity, relevance, helpfulness, coherence, appropriateness
- **Weighted scoring system**: Configurable weights for each criterion
- **Rich statistical analysis**: Mean, median, standard deviation, confidence intervals
- **Detailed qualitative feedback**: Strengths, weaknesses, and improvement suggestions

### âœ… Domain-Specific Evaluation Criteria

- **Administrative Information Disclosure**: Specialized criteria for Japan's Information Disclosure Law compliance
- **Legal Framework Integration**: Built-in evaluation for personal information, corporate confidentiality, national security, and public safety
- **Comprehensive Legal Analysis**: 6 specialized criteria covering all major non-disclosure provisions
- **Japanese Administrative Context**: Tailored for Japanese government agencies and legal requirements

### âœ… Advanced Display Capabilities

- **Rich CLI formatting**: Beautiful tables with Rich library integration
- **Comprehensive JSON output**: Structured data with all criterion details
- **Progress indicators**: Real-time feedback during evaluation processing
- **Score visualization**: Color-coded scoring and percentage displays

### âœ… Flexible Evaluation Modes

- **Multi-criteria by default**: Comprehensive 7-dimension analysis
- **Single-criterion mode**: Backward compatible with `--single-criterion` flag
- **Custom criteria support**: Adapt to different evaluation contexts
- **Batch multi-criteria**: All batch operations use comprehensive evaluation

### âœ… Structured Output Support

- **Provider-native structured output**: Uses each provider's JSON schema capabilities
- **Guaranteed response format**: Consistent JSON structure across all providers
- **Intelligent fallback parsing**: Handles parsing errors with sentiment analysis
- **Type-safe validation**: Enforces proper data types and value ranges

## ğŸ¯ Current Status

### âœ… Phase 2 Complete: Production-Ready Foundation

- âœ… **Real LLM API integration** (OpenAI GPT-4, Anthropic Claude)
- âœ… **Robust error handling** and retry logic with circuit breakers
- âœ… **Configuration management** with hierarchical loading
- âœ… **Timeout management** and request cancellation
- âœ… **Fallback mechanisms** and degraded service modes
- âœ… **Complete test suite** - 236/236 tests passing
- âœ… **Enhanced resilience patterns** and error classification
- âœ… **Comprehensive CLI interface** with evaluation and comparison commands

### âœ… Phase 3 Complete: Advanced Features

- âœ… **Multi-criteria evaluation by default**: Comprehensive 7-dimension analysis
- âœ… **Advanced batch processing**: Multi-criteria evaluation in batch operations
- âœ… **Rich CLI interface**: Beautiful formatted output with tables and progress indicators
- âœ… **Robust JSON parsing**: Multiple fallback strategies for reliable LLM response processing
- âœ… **Comprehensive error handling**: Validation, fallbacks, and graceful degradation
- âœ… **Multi-format file support**: JSONL, CSV, JSON with intelligent parsing
- âœ… **Domain-driven evaluation models**: Proper separation of criteria, scoring, and result aggregation

## ğŸ§ª Testing

**ğŸ‰ ALL 236 TESTS PASSING - Complete Test Suite Reliability**

```bash
# Run all tests (236/236 passing)
pytest

# Run specific test suites
pytest tests/unit/                    # Unit tests only
pytest tests/integration/             # Integration tests only
pytest tests/unit/infrastructure/     # Infrastructure layer tests

# Run with coverage
pytest --cov=src/llm_judge --cov-report=html
```

## ğŸš€ Usage Examples

### Administrative Information Disclosure Evaluation (Featured)

The system includes specialized criteria for evaluating administrative information disclosure decisions according to Japan's Information Disclosure Law (Act No. 42 of 1999):

```python
from src.llm_judge import LLMJudge, CandidateResponse
import asyncio

async def administrative_evaluation():
    judge = LLMJudge()

    # Load administrative information disclosure criteria
    candidate = CandidateResponse(
        prompt="ã“ã®è¡Œæ”¿æ–‡æ›¸ã®é–‹ç¤ºå¯å¦ã«ã¤ã„ã¦åˆ¤æ–­ã—ã¦ãã ã•ã„ã€‚æ–‡æ›¸ã«ã¯å€‹äººã®åŒ»ç™‚è¨˜éŒ²ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚",
        response="ã“ã®æ–‡æ›¸ã¯å€‹äººæƒ…å ±ä¿è­·æ³•ç¬¬5æ¡1å·ã«è©²å½“ã™ã‚‹ãŸã‚ã€åŸå‰‡ã¨ã—ã¦éé–‹ç¤ºã¨åˆ¤æ–­ã—ã¾ã™ã€‚ãŸã ã—ã€ç”Ÿå‘½ãƒ»å¥åº·ãƒ»ç”Ÿæ´»ãƒ»è²¡ç”£ã®ä¿è­·ã«å¿…è¦ãªå ´åˆã¯é–‹ç¤ºä¾‹å¤–ã‚’æ¤œè¨ã—ã¾ã™ã€‚",
        model="claude-3"
    )

    # Evaluate using administrative criteria
    result = await judge.evaluate_multi_criteria(
        candidate,
        criteria_file="criteria/administrative_information_non_disclosure.json"
    )

    print(f"Overall Score: {result.aggregated.overall_score:.1f}/5")
    print("Legal Analysis:")

    # Access specialized criterion scores
    for cs in result.criterion_scores:
        print(f"  {cs.criterion_name}: {cs.score}/5")
        print(f"    Reasoning: {cs.reasoning}")

    await judge.close()

asyncio.run(administrative_evaluation())
```

**Administrative Criteria Features:**

- **Personal Information Protection** (25%): Comprehensive evaluation of privacy rights
- **Corporate Information Protection** (20%): Business confidentiality assessment
- **National Security Concerns** (20%): Security and international relations evaluation
- **Public Safety and Order** (15%): Law enforcement and public safety assessment
- **Internal Deliberation Protection** (10%): Government process integrity evaluation
- **Administrative Operations Protection** (10%): Operational efficiency assessment

### Basic Multi-Criteria Evaluation

```python
from src.llm_judge import LLMJudge, CandidateResponse
import asyncio

async def basic_evaluation():
judge = LLMJudge()

candidate = CandidateResponse(
    prompt="What is AI?",
    response="AI is artificial intelligence",
    model="gpt-4"
)

    # Multi-criteria evaluation (default)
    result = await judge.evaluate_multi_criteria(candidate)

    print(f"Overall Score: {result.aggregated.overall_score:.1f}/5")
    print(f"Criteria Count: {len(result.criterion_scores)}")

    # Access individual criterion scores
    for cs in result.criterion_scores:
    print(f"{cs.criterion_name}: {cs.score}/5 - {cs.reasoning}")

    await judge.close()

asyncio.run(basic_evaluation())
```

### Pairwise Comparison

```python
async def compare_responses():
    judge = LLMJudge()

candidate_a = CandidateResponse(
    prompt="Explain quantum computing",
    response="Quantum computing uses quantum bits in superposition.",
    model="gpt-4"
)

candidate_b = CandidateResponse(
    prompt="Explain quantum computing",
    response="Quantum computers leverage quantum phenomena like superposition and entanglement.",
    model="claude-3"
)

    # Compare responses
result = await judge.compare_responses(candidate_a, candidate_b)
print(f"Winner: {result['winner']}")           # 'A', 'B', or 'tie'
print(f"Reasoning: {result['reasoning']}")     # Detailed comparison analysis
print(f"Confidence: {result['confidence']}")   # Confidence score 0-1

await judge.close()

asyncio.run(compare_responses())
```

## ğŸ”§ Configuration

### Criteria Configuration

The system now uses a unified criteria configuration system with the `criteria/` directory:

```
criteria/
â”œâ”€â”€ README.md                                    # Criteria documentation
â”œâ”€â”€ default.json                                 # Default evaluation criteria
â”œâ”€â”€ administrative_information_non_disclosure.json  # Featured: Administrative disclosure evaluation
â”œâ”€â”€ custom.json                                  # Custom criteria example
â””â”€â”€ template.json                                # Template for creating new criteria
```

#### Featured: Administrative Information Disclosure Criteria

The `administrative_information_non_disclosure.json` file provides comprehensive evaluation criteria for Japan's Information Disclosure Law (Act No. 42 of 1999):

- **Legal Compliance**: Evaluates responses against all 6 major non-disclosure provisions
- **Detailed Analysis**: Each criterion includes specific evaluation steps and legal considerations
- **Japanese Context**: Tailored for Japanese administrative agencies and legal framework
- **Comprehensive Coverage**: Personal information, corporate confidentiality, national security, public safety, internal deliberation, and administrative operations

### Environment Configuration

Create a `.env` file in the project root:

```bash
# LLM Provider API Keys
OPENAI_API_KEY=your-openai-api-key-here
ANTHROPIC_API_KEY=your-anthropic-api-key-here

# Default Provider (openai or anthropic)
DEFAULT_PROVIDER=anthropic

# Model Configuration
OPENAI_MODEL=gpt-4
ANTHROPIC_MODEL=claude-sonnet-4-20250514

# Timeout Configuration (seconds)
REQUEST_TIMEOUT=30
CONNECT_TIMEOUT=10

# Logging Level (DEBUG, INFO, WARNING, ERROR)
LOG_LEVEL=INFO
```

## ğŸ¤ Contributing

This project follows a structured development approach based on Domain-Driven Design:

1. **Documentation-First**: Review planning docs in `docs/` before coding
2. **Layer Separation**: Respect the DDD architecture boundaries
3. **Test-Driven**: Write tests before implementation
4. **Incremental**: Small, working increments with rollback capability

See [Development Guide](docs/development/README.md) for detailed contributing guidelines.

## ğŸ“„ License

MIT License - Open source implementation following the Evidently AI methodology.

## ğŸ›ï¸ Administrative Information Disclosure Evaluation

### Use Cases

The `administrative_information_non_disclosure.json` criteria file is specifically designed for:

- **Government Agencies**: Evaluating information disclosure decisions according to Japan's Information Disclosure Law
- **Legal Compliance**: Ensuring proper application of non-disclosure provisions
- **Training Programs**: Educating staff on proper disclosure evaluation procedures
- **Quality Assurance**: Standardizing disclosure decision-making processes
- **Audit and Review**: Assessing the quality of existing disclosure determinations

### Key Benefits

- **Legal Accuracy**: Built-in evaluation against all 6 major non-disclosure provisions of Japan's Information Disclosure Law
- **Comprehensive Analysis**: Detailed evaluation steps for each criterion with specific legal considerations
- **Japanese Context**: Tailored for Japanese administrative agencies and legal framework
- **Risk Assessment**: Identifies potential legal and operational risks in disclosure decisions
- **Documentation Support**: Provides structured reasoning for disclosure determinations

### Example Applications

```bash
# Evaluate a disclosure decision
python -m src.llm_judge evaluate \
  "ã“ã®æ–‡æ›¸ã®é–‹ç¤ºå¯å¦ã«ã¤ã„ã¦åˆ¤æ–­ã—ã¦ãã ã•ã„" \
  "å€‹äººæƒ…å ±ãŒå«ã¾ã‚Œã¦ã„ã‚‹ãŸã‚éé–‹ç¤ºã¨åˆ¤æ–­ã—ã¾ã™" \
  --criteria-file criteria/administrative_information_non_disclosure.json

# Batch evaluation of multiple decisions
python -m src.llm_judge batch disclosure_decisions.jsonl \
  --output evaluation_results.json \
  --criteria-file criteria/administrative_information_non_disclosure.json
```

## ğŸ™ Acknowledgments

- [Evidently AI](https://www.evidentlyai.com/) for the LLM-as-a-Judge methodology
- OpenAI and Anthropic for LLM APIs
- The Domain-Driven Design community for architectural guidance
- The open-source community for tools and inspiration

---

**Need help?** Check out our [documentation](docs/) or [open an issue](https://github.com/superluminal-jp/llm-as-a-judge/issues).
