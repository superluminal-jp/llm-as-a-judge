# LLM-as-a-Judge System

A minimal implementation of an LLM-as-a-Judge system for evaluating language model outputs, based on the [Evidently AI guide](https://www.evidentlyai.com/llm-guide/llm-as-a-judge).

## Quick Start

```bash
python llm_judge_simple.py
```

## Current Status

‚úÖ **Phase 1 Complete**: Working Minimal Implementation (`llm_judge_simple.py`)
- ‚úÖ Direct scoring evaluation (1-5 scale) with structured reasoning
- ‚úÖ Pairwise comparison (A vs B vs tie) with dimensional analysis
- ‚úÖ Mock LLM integration for development and testing
- ‚úÖ Command-line demo interface with examples
- ‚úÖ Comprehensive planning documentation across all architectural levels

üü° **Phase 2 In Progress**: Production-Ready Foundation
- üü° Real LLM API integration (OpenAI GPT-4, Anthropic Claude)
- ‚è≥ Robust error handling and retry logic with circuit breakers
- ‚è≥ Configuration management with hierarchical loading
- ‚è≥ Async processing architecture for performance
- ‚è≥ Professional CLI interface with progress tracking

## Implementation Documentation

### üìã Layered Planning Strategy

Following the AI Coding Agent Governance Framework, we maintain sophisticated documentation across multiple abstraction levels:

| Document | Abstraction Level | Purpose | Primary Audience |
|----------|-------------------|---------|------------------|
| [STRATEGY.md](STRATEGY.md) | **Strategic** | Business vision, market analysis, success metrics | C-Suite, Product Strategy, Investors |
| [ARCHITECTURE.md](ARCHITECTURE.md) | **Architectural** | System design, component relationships, scalability | Solution Architects, Technical Leadership |
| [DOMAIN-MODEL.md](DOMAIN-MODEL.md) | **Domain** | Business concepts, ubiquitous language, domain rules | Domain Experts, Senior Developers |
| [IMPLEMENTATION.md](IMPLEMENTATION.md) | **Implementation** | Detailed technical execution with acceptance criteria | Development Teams, DevOps Engineers |
| [TASKS.md](TASKS.md) | **Task** | Current sprint breakdown with validation gates | Individual Developers, Scrum Masters |
| [CODE_PLAN.md](CODE_PLAN.md) | **Code** | Specific code structure and implementation examples | Junior/Mid-level Developers |

### üéØ Documentation Synchronization Status

| Document | Last Updated | Sync Status | Code Alignment |
|----------|-------------|-------------|----------------|
| STRATEGY.md | Current | ‚úÖ Synchronized | Reflects current business objectives |
| ARCHITECTURE.md | Current | ‚úÖ Synchronized | Matches planned system design |
| DOMAIN-MODEL.md | Current | ‚úÖ Synchronized | Captures current domain understanding |
| IMPLEMENTATION.md | Current | ‚úÖ Synchronized | Detailed Phase 1 completion, Phase 2 planning |
| TASKS.md | Current | ‚úÖ Synchronized | Active sprint tasks with validation gates |
| CODE_PLAN.md | Current | ‚úÖ Synchronized | Matches current code structure |
| README.md | Current | ‚úÖ Synchronized | Reflects actual system state and capabilities |

### üéØ Development Phases

#### Phase 1: MVP ‚úÖ COMPLETE
- ‚úÖ Single-file working implementation
- ‚úÖ Core evaluation methods
- ‚úÖ Demo functionality

#### Phase 2: Production Foundation (Next)
- [ ] Real LLM API integration (OpenAI, Anthropic)
- [ ] Error handling and retry logic
- [ ] Configuration management
- [ ] Data persistence
- [ ] Enhanced CLI interface

#### Phase 3: Advanced Features
- [ ] Batch processing
- [ ] REST API
- [ ] Reference-based evaluation
- [ ] Custom evaluation criteria
- [ ] Analytics dashboard

#### Phase 4: Enterprise Scale
- [ ] Multi-tenancy
- [ ] High availability
- [ ] Advanced security
- [ ] Performance optimization
- [ ] Integration ecosystem

## Architecture Overview

```
Current: Single File (100 LOC)
‚îî‚îÄ‚îÄ llm_judge_simple.py

Target: Modular Architecture
‚îú‚îÄ‚îÄ llm_judge/
‚îÇ   ‚îú‚îÄ‚îÄ core/          # Domain models and logic
‚îÇ   ‚îú‚îÄ‚îÄ clients/       # LLM provider integrations
‚îÇ   ‚îú‚îÄ‚îÄ storage/       # Data persistence
‚îÇ   ‚îú‚îÄ‚îÄ api/           # REST API server
‚îÇ   ‚îî‚îÄ‚îÄ cli/           # Command-line interface
‚îú‚îÄ‚îÄ tests/             # Test suites
‚îî‚îÄ‚îÄ docs/              # Documentation
```

## Key Features

### Evaluation Methods
- **Direct Scoring**: Rate responses on 1-5 scale with reasoning
- **Pairwise Comparison**: Compare two responses (A vs B vs tie)
- **Reference-Based**: Compare against golden examples *(planned)*

### LLM Provider Support
- **Current**: Mock integration for testing
- **Planned**: OpenAI GPT-4, Anthropic Claude, Local models

### Evaluation Criteria
- **Current**: Single criteria (e.g., "overall quality")
- **Planned**: Multi-dimensional evaluation, custom criteria

## Usage Examples

### Current Simple Usage
```python
from llm_judge_simple import LLMJudge, CandidateResponse

judge = LLMJudge()

# Single evaluation
candidate = CandidateResponse(
    prompt="What is AI?",
    response="AI is artificial intelligence",
    model="gpt-3.5"
)

result = judge.evaluate_response(candidate, "accuracy and clarity")
print(f"Score: {result.score}/5")
print(f"Reasoning: {result.reasoning}")
```

### Planned Enhanced Usage
```python
# Real LLM integration (Phase 2)
judge = EnhancedLLMJudge(provider="openai", api_key="sk-...")

# Async evaluation
result = await judge.evaluate_response_async(candidate)

# Batch processing (Phase 3)
results = await judge.evaluate_batch(candidates)

# API integration (Phase 3)
POST /evaluate
{
  "prompt": "What is AI?",
  "response": "AI is artificial intelligence",
  "criteria": "accuracy and clarity"
}
```

## Development Setup

### Prerequisites
- Python 3.9+
- Optional: OpenAI or Anthropic API keys for real LLM integration

### Installation
```bash
# Current minimal version
# No installation needed - single file

# Future modular version (Phase 2+)
pip install -e .
pip install -r requirements-dev.txt
```

### Testing
```bash
# Current
python llm_judge_simple.py

# Future
pytest tests/
```

## Contributing

This project follows a structured development approach:

1. **Start with documentation**: Review planning docs before coding
2. **Follow phases**: Complete Phase 1 before starting Phase 2
3. **Test-driven**: Write tests before implementation
4. **Incremental**: Small, working increments with rollback capability

## Roadmap

| Milestone | Timeline | Key Deliverables |
|-----------|----------|------------------|
| Phase 1 | ‚úÖ Complete | Working minimal implementation |
| Phase 2 | 2-4 weeks | Production-ready core with real LLM integration |
| Phase 3 | 6-8 weeks | Advanced features and API |
| Phase 4 | 3-6 months | Enterprise-scale deployment |

## License

MIT License - Open source implementation following the Evidently AI methodology.

## Acknowledgments

- [Evidently AI](https://www.evidentlyai.com/) for the LLM-as-a-Judge methodology
- OpenAI and Anthropic for LLM APIs
- The open-source community for tools and inspiration